---
---
@tutorial{pfn,
    title = {Prior-Fitted Networks},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2112.10510" target="_blank">Müller et al. Transformers can do Bayesian Inference. ICLR, 2022</a><br>
    <b>Task</b>: Using Transformers to estimate Posterior Predictive Distributions<br>
    <b>Libraries</b>: PyTorch<br>
    <b>Learning objectives</b>: <br>
    <ul>
      <li>Understand the principles of Prior-Data Fitted Networks (PFNs) and their application in integrating prior knowledge to predict the Posterior Predictive Distribution (PPD) in machine learning models.</li>
      <li>Acquire practical skills in defining priors, creating dataset loaders for synthetic data generation, developing transformer models for PPD approximation, formulating loss functions for specific regression tasks, and evaluating model output quality.</li>
    </ul>
    },
    image={../images/resources/pfn.png},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-final.ipynb},
}

@tutorial{pfn_data_priors,
    title = {Data Generating Priors for Prior-Fitted Networks},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Müller et al. Transformers can do Bayesian Inference. ICLR, 2022</a><br><a href="https://arxiv.org/abs/2207.01848" target="_blank">Hollmann et al. TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. ICLR, 2023</a><br>
    <b>Task</b>: Generating data according to different priors for PFNs<br>
    <b>Libraries</b>: PyTorch<br>
    <b>Learning objectives</b>: Learn how to generate synthetic data based on specified priors and utilize it for training neural networks to approximate Bayesian inference.<br>
    },

    image={../images/resources/pfn_data.png},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-priors.ipynb},
}

@tutorial{llm_time_series_forecasting,
    title = {LLMTime - Zero-shot prompting LLMs for time series forecasting},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Gruver et al. Large Language Models are Zero Shot Time Series Forecasters. NeurIPS, 2023</a><br>
    <b>Task</b>: Weather forecasting using LLMs<br>
    <b>Libraries</b>: openai, tiktoken, jax<br><br>
    <b>Learning objectives</b>: Explore zero-shot prompting with Large Language Models (LLMs) for time series forecasting. In this tutorial, we aim to:<br>
    <ul>
      <li>Acquaint you with the application of machine learning techniques using Large Language Models (LLMs).</li>
      <li>Enhance your understanding of LLMs and the parameters influencing their behavior.</li>
      <li>Guide you through the essentials for successful time series prediction with LLMs.</li>
      <li>Translate knowledge from transformers to the realm of LLMs.</li>
    </ul>
    },

    image={../images/resources/llmtime.png},
    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries-solution.ipynb},
}

@tutorial{attn,
    title = {Attention is all you need},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Vaswani, Ashish, et al. "Attention is all you need." NeurIPS 2017.</a><br>
    <b>Task</b>: Neural Machine Translation (e.g, German-English)<br>
    <b>Dataset</b>: <a href="https://arxiv.org/abs/1605.00459" target="_blank">Multi30k</a><br>
    <b>Libraries</b>: PyTorch, NLTK, Spacy, torchtext <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Build a transformer model for neural machine translation</li>
      <li>Train the model using proposed label smoothing loss and learning rate scheduler</li>
      <li>Use the trained model to infer likely translations using </li>
        <ul>
          <li>Greedy Decoding</li>
          <li>Beam Search</li>
        </ul>
    </ul>
    },
    image={../images/resources/transformer_architecture.jpeg},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical-solution.ipynb},

}

@tutorial{critical_exploration_transformers,
    title = {Critical Exploration of Transformer Models},
    objective = {
    <b>Learning objectives</b>: Delve into the inner workings of transformer models beyond basic applications<br>
    <b>Key Areas</b>:
    <ul>
      <li>Adversarial Inputs: Crafting inputs to challenge language models</li>
      <li>Attention Visualization: Understanding focus mechanisms in transformers</li>
      <li>Fine-Tuning with LoRA: Implementing Low-Rank Adaptation for model refinement</li>
      <li>Bias Detection: Investigating biases in model responses</li>
    </ul>
    <b>Note</b>: This tutorial is an introductory exploration of transformers. It's a starting point for more advanced study.<br><br>
    },
    image={../images/resources/lora.png},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended-solution.ipynb},
}

@tutorial{mat,
    title = {Molecule Attention Transformer},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/pdf/2002.08264.pdf" target="_blank">Maziarka, et al. "Molecule Attention Transformer"</a><br>
    <b>Task</b>: Classification task to predict Blood-brain barrier permeability (BBBP)<br>
    <b>Dataset</b>: <a href="https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html#bbbp-datasets" target="_blank">BBBP</a><br>
    <b>Libraries</b>: PyTorch, DeepChem, RDKit <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Learn key concepts required to work with molecules</li>
      <li>Perform critical data preprocessing tasks, such as feature extraction, graph formation, and scaffold splitting</li>
      <li>Explore challenges of drug discovery, particularly designing drugs that can cross the blood-brain barrier and enter the central nervous system</li>
      <li>Implement Molecule Attention Transformer (MAT) proposed by Maziarka et al. (2020) using a deep learning pipeline</li>
      <li>Train and evaluate the model on molecular datasets</li>
    </ul>
    },
    image={../images/resources/MAT.png},

    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/molecule_attention_transformer/practical-solution.ipynb},

}

@tutorial{ard,
    title = {Accessing {R}esearch {D}ata for {S}ocial {S}cience [Oxford Internet Institute, MT 2022]},
    objective = {
    <b>DeepNote</b>: (Jupyter notebook hosting service) <a href="https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA">DIY Notebooks</a>.<br>
    <b>Github</b>: <a href="https://github.com/pg2455/accessing-research-data">Repository</a> to work on your local machine.<br>
    <b>Programming Language</b>: Python<br>
    <b>Libraries</b>: Pandas, feedparser, newscatcherapi, psaw, requests, twarc (Twitter API), requests-html<br>

    <b>Learning objectives</b>:
    <ul>
      <li>Use Python to collect research data from the social web</li>
      <li>Give due consideration to the ethics of data collection</li>
      <li> Following topics are covered:</li>
      <ul>
        <li>Accessing RSS feeds</li>
        <li>Accessing Reddit and Wikipedia through APIs</li>
        <li>Accessing Twitter using Twitter API</li>
        <li>Web crawling</li>
      </ul>
    </ul>
    },

    image={../images/resources/ard.png},
    link_diy = {https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA},


}
