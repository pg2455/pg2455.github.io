---
---

@tutorial{attn,
    title = {Attention is all you need},
    objective = {
    <b>Paper</b>: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Vaswani, Ashish, et al. "Attention is all you need." NeurIPS 2017.</a><br>
    <b>Task</b>: Neural Machine Translation (e.g, German-English)<br>
    <b>Dataset</b>: <a href="https://arxiv.org/abs/1605.00459" target="_blank">Multi30k</a><br>
    <b>Libraries</b>: PyTorch, NLTK, Spacy, torchtext <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Build a transformer model for neural machine translation</li>
      <li>Train the model using proposed label smoothing loss and learning rate scheduler</li>
      <li>Use the trained model to infer likely translations using </li>
        <ul>
          <li>Greedy Decoding</li>
          <li>Beam Search</li>
        </ul>
    </ul>
    },

    image={../images/resources/transformer_architecture.jpeg},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical-solution.ipynb},

}

@tutorial{mat,
    title = {Molecule Attention Transformer},
    objective = {
    <b>Paper</b>: <a href="https://arxiv.org/pdf/2002.08264.pdf" target="_blank">Maziarka, et al. "Molecule Attention Transformer"</a><br>
    <b>Task</b>: Classification task to predict Blood-brain barrier permeability (BBBP)<br>
    <b>Dataset</b>: <a href="https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html#bbbp-datasets" target="_blank">BBBP</a><br>
    <b>Libraries</b>: PyTorch, DeepChem, RDKit <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Learn key concepts required to work with molecules</li>
      <li>Perform critical data preprocessing tasks, such as feature extraction, graph formation, and scaffold splitting</li>
      <li>Explore challenges of drug discovery, particularly designing drugs that can cross the blood-brain barrier and enter the central nervous system</li>
      <li>Implement Molecule Attention Transformer (MAT) proposed by Maziarka et al. (2020) using a deep learning pipeline</li>
      <li>Train and evaluate the model on molecular datasets</li>
    </ul>
    },

    image={../images/resources/MAT.png},

    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/molecule_attention_transformer/practical-solution.ipynb},

}

@tutorial{ard,
    title = {Accessing {R}esearch {D}ata for {S}ocial {S}cience [Oxford Internet Institute, MT 2022]},
    objective = {
    <b>DeepNote</b>: (Jupyter notebook hosting service) <a href="https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA">DIY Notebooks</a>.<br>
    <b>Github</b>: <a href="https://github.com/pg2455/accessing-research-data">Repository</a> to work on your local machine.<br>
    <b>Programming Language</b>: Python<br>
    <b>Libraries</b>: Pandas, feedparser, newscatcherapi, psaw, requests, twarc (Twitter API), requests-html<br>

    <b>Learning objectives</b>:
    <ul>
      <li>Use Python to collect research data from the social web</li>
      <li>Give due consideration to the ethics of data collection</li>
      <li> Following topics are covered:</li>
      <ul>
        <li>Accessing RSS feeds</li>
        <li>Accessing Reddit and Wikipedia through APIs</li>
        <li>Accessing Twitter using Twitter API</li>
        <li>Web crawling</li>
      </ul>
    </ul>
    },

    image={../images/resources/ard.png},
    link_diy = {https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA},


}
