---
---
# I'll create a tutorial entry for your time series forecasting series in the same format as the existing entries

@tutorial{time_series_forecasting,
    title = {Time Series Forecasting: Classical to Advanced Modeling Techniques & Tools},
    objective = {
    <b>Task</b>: Building understanding of time series forecasting problems, challenges, and various techniques through hands-on Python notebooks<br>
    <b>Libraries</b>: GluonTS, neuralforecast, Chronos, PyTorch, NumPy, Matplotlib<br>
    <b>Series Overview</b>: This comprehensive tutorial series covers time series forecasting from classical methods to advanced LLM-based approaches through 7 modules:<br>
      <ul>
        <li><b>Module 1-2</b> - Covers the basics of time series forecasting, including introduction to GluonTS</li>
        <li><b>Module 3</b> - Focuses on data and problem formulation for time series modeling</li>
        <li><b>Module 4-5</b> - Explores classical and neural network-based forecasting approaches</li>
        <li><b>Module 6</b> - Covers transformer-based architectures for time series</li>
        <li><b>Module 7</b> - Introduces LLM-based approaches for time series forecasting</li>
      </ul>
    },
    image={../images/resources/lstnet.png},
    date_published={September, 2024},
    link_sol={https://github.com/pg2455/time_series_forecasting_tutorial}
}

@tutorial{kan_tutorial_series,
    title = {Understanding Kolmogorov-Arnold Networks: A Tutorial Series on KAN using Toy Examples},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2404.19756" target="_blank">Liu et al. KAN: Kolmogorov-Arnold Networks. ArXiv, 2024</a><br>
    <b>Task</b>: Building an understanding of Kolmogorov-Arnold Networks (KAN) using B-splines as activation functions through practical coding examples and theoretical insights.<br>
    <b>Libraries</b>: PyTorch, NumPy, Matplotlib, Jupyter<br>
    <b>Series Overview</b>: This tutorial series guides you through the complexities of KAN by dissecting its core components and demonstrating their application through practical examples. Each notebook in the series focuses on a different aspect of KANs:<br>
      <ul>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/1_splines.ipynb" target="_blank">B-Splines for KAN</a></b> - Introduces B-splines and their role in KANs.</li>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/2_stacked_splines.ipynb" target="_blank">Deeper KANs</a></b> - Explores network construction and backpropagation in deeper KAN architectures.</li>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/3_grids.ipynb" target="_blank">Grid Manipulation in KANs</a></b> - Discusses model capacity expansion and continual learning without catastrophic forgetting.</li>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/4_symbolic_learning.ipynb" target="_blank">Symbolic Regression using KANs</a></b> - Teaches integration of symbolic regression within KAN networks.</li>
      </ul>
    },
    image={../images/resources/kan.png},
    date_published={May 27, 2024},
    link_sol={https://github.com/pg2455/KAN-Tutorial}
}

@tutorial{llm_tranmission_chains,
    title = {Exploring LLM Transmission Chains: A Tutorial Series on Human-like Content Biases in LLMs<br>},
    objective = {
    <b>Publication</b>: <a href="https://www.pnas.org/doi/10.1073/pnas.2313790120" target="_blank">Acerbi et al. Large Language Models Show Human-like Content Biases in Transmission Chain Experiments. PNAS, 2023</a><br>
    <b>Task</b>: Reproducing and understanding the results regarding content biases in LLMs as observed in transmission chain experiments<br>
    <b>Libraries</b>: openai, rpy2, python-docx, matplotlib, pandas<br>
    <b>Series Overview</b>: This series includes multiple detailed notebooks, guiding you through the process of setting up, executing, and analyzing transmission chain experiments with LLMs to explore human-like content biases. Each notebook corresponds to one of the six plots in the main paper and includes three sections:<br>
      <ul>
        <li><b>Authors' Results</b> - Reproduces the paper's findings using Python (originally conducted in R).</li>
        <li><b>Authors' Summaries / GPT-4 Evaluation</b> - Evaluates summaries from the original study using GPT-4.</li>
        <li><b>New Summaries / GPT-4 Evaluation</b> - Uses new summaries from updated experiments and evaluates them using GPT-4.</li>
      </ul>
    },
    image={../images/resources/llm-chains.png},
    date_published={April 30, 2024},
    link_sol={https://github.com/pg2455/LLM-Transmission-Chains}
}

@tutorial{simulacra_series,
    title = {Exploring Simulacra: A Tutorial Series on Simulacra's Generative Agents},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2304.03442" target="_blank">Park et al. Generative Agents: Interactice Simulacra of Human Behavior. UIST, 2023</a><br>
    <b>Task</b>: Creating and understanding generative agents proposed in Simulacra<br>
    <b>Libraries</b>: openai<br>
    <b>Series Overview</b>: This tutorial series spans four comprehensive notebooks, each designed to progressively build your understanding and skills in simulating sociological phenomena with generative agents. <a href="/blog/2024/simulacra-0/" target="_blank">Check out the blog post</a> where I break down the big ideas behind creating sociological simulations in an easy-to-get way.<br>
    <ul>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.1.ipynb" target="_blank">Notebook 1.1</a> - Basic Simulation Structure</b>: An introduction to the essential components of simulation, including environment interaction and the construction of a basic simulation loop.</li>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.2.ipynb" target="_blank">Notebook 1.2</a> - Building Generative Agents</b>: A deep dive into the creation of generative agents, with emphasis on memory structures derived from the Simulacra paper.</li>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.3.ipynb" target="_blank">Notebook 1.3</a> - Agent Schedule Planning</b>: Techniques for developing agents' abilities to autonomously plan their schedules.</li>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.4.ipynb" target="_blank">Notebook 1.4</a> - Agent Cognition and Communication</b>: Exploration of agents' cognitive functions, such as perception and memory retrieval, and how they interact and communicate.</li>
    </ul>
    },
    image={../images/blog/simulacra/simulacra.png},
    date_published={March 31, 2024},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-final.ipynb},
}

@tutorial{pfn,
    title = {Prior-Fitted Networks},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2112.10510" target="_blank">Müller et al. Transformers can do Bayesian Inference. ICLR, 2022</a><br>
    <b>Task</b>: Using Transformers to estimate Posterior Predictive Distributions<br>
    <b>Libraries</b>: PyTorch<br>
    <b>Learning objectives</b>: <br>
    <ul>
      <li>Understand the principles of Prior-Data Fitted Networks (PFNs) and their application in integrating prior knowledge to predict the Posterior Predictive Distribution (PPD) in machine learning models.</li>
      <li>Acquire practical skills in defining priors, creating dataset loaders for synthetic data generation, developing transformer models for PPD approximation, formulating loss functions for specific regression tasks, and evaluating model output quality.</li>
    </ul>
    },
    image={../images/resources/pfn.png},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-final.ipynb},
    date_published={Feb 5, 2024},
}

@tutorial{pfn_data_priors,
    title = {Data Generating Priors for Prior-Fitted Networks},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Müller et al. Transformers can do Bayesian Inference. ICLR, 2022</a><br><a href="https://arxiv.org/abs/2207.01848" target="_blank">Hollmann et al. TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. ICLR, 2023</a><br>
    <b>Task</b>: Generating data according to different priors for PFNs<br>
    <b>Libraries</b>: PyTorch<br>
    <b>Learning objectives</b>: Learn how to generate synthetic data based on specified priors and utilize it for training neural networks to approximate Bayesian inference.<br>
    },

    image={../images/resources/pfn_data.png},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-priors.ipynb},
    date_published={Feb 5, 2024},
}

@tutorial{llm_time_series_forecasting,
    title = {LLMTime - Zero-shot prompting LLMs for time series forecasting},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Gruver et al. Large Language Models are Zero Shot Time Series Forecasters. NeurIPS, 2023</a><br>
    <b>Task</b>: Weather forecasting using LLMs<br>
    <b>Libraries</b>: openai, tiktoken, jax<br><br>
    <b>Learning objectives</b>: Explore zero-shot prompting with Large Language Models (LLMs) for time series forecasting. In this tutorial, we aim to:<br>
    <ul>
      <li>Acquaint you with the application of machine learning techniques using Large Language Models (LLMs).</li>
      <li>Enhance your understanding of LLMs and the parameters influencing their behavior.</li>
      <li>Guide you through the essentials for successful time series prediction with LLMs.</li>
      <li>Translate knowledge from transformers to the realm of LLMs.</li>
    </ul>
    },

    image={../images/resources/llmtime.png},
    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries-solution.ipynb},
    date_published={Nov 12, 2023},
}

@tutorial{critical_exploration_transformers,
    title = {Critical Exploration of Transformer Models},
    objective = {
    <b>Learning objectives</b>: Delve into the inner workings of transformer models beyond basic applications<br>
    <b>Key Areas</b>:
    <ul>
      <li>Adversarial Inputs: Crafting inputs to challenge language models</li>
      <li>Attention Visualization: Understanding focus mechanisms in transformers</li>
      <li>Fine-Tuning with LoRA: Implementing Low-Rank Adaptation for model refinement</li>
      <li>Bias Detection: Investigating biases in model responses</li>
    </ul>
    <b>Note</b>: This tutorial is an introductory exploration of transformers. It's a starting point for more advanced study.<br><br>
    },
    image={../images/resources/lora.png},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended-solution.ipynb},
    date_published={Nov 7, 2023},
}

@tutorial{mat,
    title = {Molecule Attention Transformer},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/pdf/2002.08264.pdf" target="_blank">Maziarka, et al. "Molecule Attention Transformer"</a><br>
    <b>Task</b>: Classification task to predict Blood-brain barrier permeability (BBBP)<br>
    <b>Dataset</b>: <a href="https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html#bbbp-datasets" target="_blank">BBBP</a><br>
    <b>Libraries</b>: PyTorch, DeepChem, RDKit <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Learn key concepts required to work with molecules</li>
      <li>Perform critical data preprocessing tasks, such as feature extraction, graph formation, and scaffold splitting</li>
      <li>Explore challenges of drug discovery, particularly designing drugs that can cross the blood-brain barrier and enter the central nervous system</li>
      <li>Implement Molecule Attention Transformer (MAT) proposed by Maziarka et al. (2020) using a deep learning pipeline</li>
      <li>Train and evaluate the model on molecular datasets</li>
    </ul>
    },
    image={../images/resources/MAT.png},

    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/molecule_attention_transformer/practical-solution.ipynb},
    date_published={Apr 21, 2023},

}

@tutorial{ard,
    title = {Accessing {R}esearch {D}ata for {S}ocial {S}cience [Oxford Internet Institute, MT 2022]},
    objective = {
    <b>DeepNote</b>: (Jupyter notebook hosting service) <a href="https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA">DIY Notebooks</a>.<br>
    <b>Github</b>: <a href="https://github.com/pg2455/accessing-research-data">Repository</a> to work on your local machine.<br>
    <b>Programming Language</b>: Python<br>
    <b>Libraries</b>: Pandas, feedparser, newscatcherapi, psaw, requests, twarc (Twitter API), requests-html<br>

    <b>Learning objectives</b>:
    <ul>
      <li>Use Python to collect research data from the social web</li>
      <li>Give due consideration to the ethics of data collection</li>
      <li> Following topics are covered:</li>
      <ul>
        <li>Accessing RSS feeds</li>
        <li>Accessing Reddit and Wikipedia through APIs</li>
        <li>Accessing Twitter using Twitter API</li>
        <li>Web crawling</li>
      </ul>
    </ul>
    },

    image={../images/resources/ard.png},
    link_diy = {https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA},
    date_published={September, 2022},
}

@tutorial{attn,
    title = {Attention is all you need},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Vaswani, Ashish, et al. "Attention is all you need." NeurIPS 2017.</a><br>
    <b>Task</b>: Neural Machine Translation (e.g, German-English)<br>
    <b>Dataset</b>: <a href="https://arxiv.org/abs/1605.00459" target="_blank">Multi30k</a><br>
    <b>Libraries</b>: PyTorch, NLTK, Spacy, torchtext <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Build a transformer model for neural machine translation</li>
      <li>Train the model using proposed label smoothing loss and learning rate scheduler</li>
      <li>Use the trained model to infer likely translations using </li>
        <ul>
          <li>Greedy Decoding</li>
          <li>Beam Search</li>
        </ul>
    </ul>
    },
    image={../images/resources/transformer_architecture.jpeg},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical-solution.ipynb},
    date_published={Aug 5, 2021},

}
