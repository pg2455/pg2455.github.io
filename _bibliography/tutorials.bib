---
---
@tutorial{llm_time_series_forecasting,
    title = {LLMTime - Zero-shot prompting LLMs for time series forecasting},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Gruver et al. Large Language Models are Zero Shot Time Series Forecasters. NeurIPS, 2023</a><br>
    <b>Task</b>: Weather forecasting using LLMs<br>
    <b>Libraries</b>: openai, tiktoken, jax<br><br>
    <b>Learning objectives</b>: Explore zero-shot prompting with Large Language Models (LLMs) for time series forecasting. In this tutorial, we aim to:<br>
    <ul>
      <li>Acquaint you with the application of machine learning techniques using Large Language Models (LLMs).</li>
      <li>Enhance your understanding of LLMs and the parameters influencing their behavior.</li>
      <li>Guide you through the essentials for successful time series prediction with LLMs.</li>
      <li>Translate knowledge from transformers to the realm of LLMs.</li>
    </ul>
    },

    image={../images/resources/llmtime.png},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries-solution.ipynb},
}

@tutorial{attn,
    title = {Attention is all you need},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Vaswani, Ashish, et al. "Attention is all you need." NeurIPS 2017.</a><br>
    <b>Task</b>: Neural Machine Translation (e.g, German-English)<br>
    <b>Dataset</b>: <a href="https://arxiv.org/abs/1605.00459" target="_blank">Multi30k</a><br>
    <b>Libraries</b>: PyTorch, NLTK, Spacy, torchtext <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Build a transformer model for neural machine translation</li>
      <li>Train the model using proposed label smoothing loss and learning rate scheduler</li>
      <li>Use the trained model to infer likely translations using </li>
        <ul>
          <li>Greedy Decoding</li>
          <li>Beam Search</li>
        </ul>
    </ul>
    },

    image={../images/resources/transformer_architecture.jpeg},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical-solution.ipynb},

}

@tutorial{critical_exploration_transformers,
    title = {Critical Exploration of Transformer Models},
    objective = {
    <b>Learning objectives</b>: Delve into the inner workings of transformer models beyond basic applications<br>
    <b>Key Areas</b>:
    <ul>
      <li>Adversarial Inputs: Crafting inputs to challenge language models</li>
      <li>Attention Visualization: Understanding focus mechanisms in transformers</li>
      <li>Fine-Tuning with LoRA: Implementing Low-Rank Adaptation for model refinement</li>
      <li>Bias Detection: Investigating biases in model responses</li>
    </ul>
    <b>Note</b>: This tutorial is an introductory exploration of transformers. It's a starting point for more advanced study.<br><br>
    },

    image={../images/resources/lora.png},

    link_diy={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended.ipynb},
    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended-solution.ipynb},
}

@tutorial{mat,
    title = {Molecule Attention Transformer},
    objective = {
    <b>Publication</b>: <a href="https://arxiv.org/pdf/2002.08264.pdf" target="_blank">Maziarka, et al. "Molecule Attention Transformer"</a><br>
    <b>Task</b>: Classification task to predict Blood-brain barrier permeability (BBBP)<br>
    <b>Dataset</b>: <a href="https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html#bbbp-datasets" target="_blank">BBBP</a><br>
    <b>Libraries</b>: PyTorch, DeepChem, RDKit <br><br>
    <b>Learning objectives</b>:
    <ul>
      <li>Learn key concepts required to work with molecules</li>
      <li>Perform critical data preprocessing tasks, such as feature extraction, graph formation, and scaffold splitting</li>
      <li>Explore challenges of drug discovery, particularly designing drugs that can cross the blood-brain barrier and enter the central nervous system</li>
      <li>Implement Molecule Attention Transformer (MAT) proposed by Maziarka et al. (2020) using a deep learning pipeline</li>
      <li>Train and evaluate the model on molecular datasets</li>
    </ul>
    },

    image={../images/resources/MAT.png},

    link_sol={https://github.com/pg2455/ml_resources/blob/master/tutorials/molecule_attention_transformer/practical-solution.ipynb},

}

@tutorial{ard,
    title = {Accessing {R}esearch {D}ata for {S}ocial {S}cience [Oxford Internet Institute, MT 2022]},
    objective = {
    <b>DeepNote</b>: (Jupyter notebook hosting service) <a href="https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA">DIY Notebooks</a>.<br>
    <b>Github</b>: <a href="https://github.com/pg2455/accessing-research-data">Repository</a> to work on your local machine.<br>
    <b>Programming Language</b>: Python<br>
    <b>Libraries</b>: Pandas, feedparser, newscatcherapi, psaw, requests, twarc (Twitter API), requests-html<br>

    <b>Learning objectives</b>:
    <ul>
      <li>Use Python to collect research data from the social web</li>
      <li>Give due consideration to the ethics of data collection</li>
      <li> Following topics are covered:</li>
      <ul>
        <li>Accessing RSS feeds</li>
        <li>Accessing Reddit and Wikipedia through APIs</li>
        <li>Accessing Twitter using Twitter API</li>
        <li>Web crawling</li>
      </ul>
    </ul>
    },

    image={../images/resources/ard.png},
    link_diy = {https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA},


}
