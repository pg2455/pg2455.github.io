<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8">
  <title>Resources</title>

  <meta name="author" content="Prateek Gupta" />
  <meta name="description" content="Prateek Gupta | Personal Website" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <link rel="alternate" type="application/rss+xml" href="/atom.xml" />

  <link href="/vendor/css/bootstrap.min.css" rel="stylesheet">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet">
  <script src="https://kit.fontawesome.com/d7c6b63fdf.js" crossorigin="anonymous"></script>
  <link href="/vendor/css/academicons.min.css" rel="stylesheet">
  <link href="/vendor/pygments/default.css" rel="stylesheet">
  <link href="/css/bamos.css" rel="stylesheet">
  <link href="/css/updates.css" rel="stylesheet">
  <link href="/css/sharingbuttons.css" rel="stylesheet">
  <link href="/css/pgupta.css" rel="stylesheet">

  <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="manifest" href="/favicon/site.webmanifest">

  <!-- MathJax -->
  

  

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>
<body>
	<div class="navbar navbar-default navbar-fixed-top">
		<div class="container">
			<div class="row">
				<div class="col-md-10 col-md-offset-1">
					<div class="navbar-header">
						  <a href="/" class="navbar-brand">
                  <div>
                      <img src="/images/me-face.png" class="img-circle"></img>
                      Prateek Gupta
                  </div>
              </a>
						<button class="navbar-toggle" type="button" data-toggle="collapse"
                    data-target="#navbar-main">
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
							<span class="icon-bar"></span>
						</button>
					</div>
					<div class="navbar-collapse collapse" id="navbar-main">
						<ul class="nav navbar-nav">
							<li>
								<a href="/">About</a>
							</li>
              <li class="dropdown">
                  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                      Teaching <span class="caret"></span>
                  </a>
                  <ul class="dropdown-menu">
                      <li><a href="/teaching/tutorials">Tutorials</a></li>
                      <li><a href="/teaching/courses">Courses</a></li>
                      <li><a href="/teaching/hackathons">Hackathons</a></li>
                  </ul>
              </li>
              
  							<li>
  								<a href="/blog/">Blog</a>
  							</li>
              
              <li>
								<a href="/more/">More</a>
							</li>
						</ul>
						<ul class="nav navbar-nav navbar-right" style="font-size: 1.5em">
							<li>
								<a href="http://github.com/pg2455" target="_blank" class="social-icon">
									<i class="fab fa-github"></i></a>
							</li>
              <li>
                <a href="https://scholar.google.com/citations?user=fvn0COgAAAAJ" target="_blank" class="social-icon">
                  <i class="ai ai-google-scholar"></i></a>
              </li>
							<li>
								<a href="http://twitter.com/pguptacs" target="_blank" class="social-icon">
									<i class="fab fa-x-twitter"></i></a>
							</li>
              <li>
                <a href="http://www.linkedin.com/in/link2prateek" target="_blank" class="social-icon">
                  <i class="fab fa-linkedin"></i></a>
              </li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</div>

  <br/>
<div class="container">
<div class="row">
<div class="col-md-10 col-md-offset-1">
  <p><br /></p>

<ul class="vcenter list-inline idxIcons" style="font-size: 1em; margin-top:1em">
  <li>
    <a href="#-tutorials" class="button1">Tutorials</a>
  </li>
  <li>
    <a href="#-courses" class="button1">Courses</a>
  </li>
  <li>
    <a href="#-hackathons" class="button1">Hackathons</a>
  </li>
</ul>

<p><br /></p>

<h2 id="-tutorials"><i class="fa fa-chevron-right"></i> Tutorials</h2>

<table class="table table-hover">

  <ol class="bibliography"><li><tr>
  <td class="col-md-3"><img src="../images/resources/lstnet.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="time-series-forecasting" class="highlight-text">
        <span> <a href="#time-series-forecasting" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Time Series Forecasting: Classical to Advanced Modeling Techniques &amp; Tools</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_time-series-forecasting&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">September, 2024</span>
    
    
    

    

    
    <div id="objective_time-series-forecasting" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/time_series_forecasting_tutorial" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Task</b>: Building understanding of time series forecasting problems, challenges, and various techniques through hands-on Python notebooks<br />
    <b>Libraries</b>: GluonTS, neuralforecast, Chronos, PyTorch, NumPy, Matplotlib<br />
    <b>Series Overview</b>: This comprehensive tutorial series covers time series forecasting from classical methods to advanced LLM-based approaches through 7 modules:<br />
      <ul>
        <li><b>Module 1-2</b> - Covers the basics of time series forecasting, including introduction to GluonTS</li>
        <li><b>Module 3</b> - Focuses on data and problem formulation for time series modeling</li>
        <li><b>Module 4-5</b> - Explores classical and neural network-based forecasting approaches</li>
        <li><b>Module 6</b> - Covers transformer-based architectures for time series</li>
        <li><b>Module 7</b> - Introduces LLM-based approaches for time series forecasting</li>
      </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/kan.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="kan-tutorial-series" class="highlight-text">
        <span> <a href="#kan-tutorial-series" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Understanding Kolmogorov-Arnold Networks: A Tutorial Series on KAN using Toy Examples</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_kan-tutorial-series&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">May 27, 2024</span>
    
    
    

    

    
    <div id="objective_kan-tutorial-series" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/KAN-Tutorial" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/abs/2404.19756" target="_blank">Liu et al. KAN: Kolmogorov-Arnold Networks. ArXiv, 2024</a><br />
    <b>Task</b>: Building an understanding of Kolmogorov-Arnold Networks (KAN) using B-splines as activation functions through practical coding examples and theoretical insights.<br />
    <b>Libraries</b>: PyTorch, NumPy, Matplotlib, Jupyter<br />
    <b>Series Overview</b>: This tutorial series guides you through the complexities of KAN by dissecting its core components and demonstrating their application through practical examples. Each notebook in the series focuses on a different aspect of KANs:<br />
      <ul>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/1_splines.ipynb" target="_blank">B-Splines for KAN</a></b> - Introduces B-splines and their role in KANs.</li>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/2_stacked_splines.ipynb" target="_blank">Deeper KANs</a></b> - Explores network construction and backpropagation in deeper KAN architectures.</li>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/3_grids.ipynb" target="_blank">Grid Manipulation in KANs</a></b> - Discusses model capacity expansion and continual learning without catastrophic forgetting.</li>
        <li><b><a href="https://github.com/pg2455/KAN-Tutorial/blob/main/4_symbolic_learning.ipynb" target="_blank">Symbolic Regression using KANs</a></b> - Teaches integration of symbolic regression within KAN networks.</li>
      </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/llm-chains.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="llm-tranmission-chains" class="highlight-text">
        <span> <a href="#llm-tranmission-chains" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Exploring LLM Transmission Chains: A Tutorial Series on Human-like Content Biases in LLMs<br /></strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_llm-tranmission-chains&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">April 30, 2024</span>
    
    
    

    

    
    <div id="objective_llm-tranmission-chains" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/LLM-Transmission-Chains" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://www.pnas.org/doi/10.1073/pnas.2313790120" target="_blank">Acerbi et al. Large Language Models Show Human-like Content Biases in Transmission Chain Experiments. PNAS, 2023</a><br />
    <b>Task</b>: Reproducing and understanding the results regarding content biases in LLMs as observed in transmission chain experiments<br />
    <b>Libraries</b>: openai, rpy2, python-docx, matplotlib, pandas<br />
    <b>Series Overview</b>: This series includes multiple detailed notebooks, guiding you through the process of setting up, executing, and analyzing transmission chain experiments with LLMs to explore human-like content biases. Each notebook corresponds to one of the six plots in the main paper and includes three sections:<br />
      <ul>
        <li><b>Authors’ Results</b> - Reproduces the paper’s findings using Python (originally conducted in R).</li>
        <li><b>Authors’ Summaries / GPT-4 Evaluation</b> - Evaluates summaries from the original study using GPT-4.</li>
        <li><b>New Summaries / GPT-4 Evaluation</b> - Uses new summaries from updated experiments and evaluates them using GPT-4.</li>
      </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/blog/simulacra/simulacra.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="simulacra-series" class="highlight-text">
        <span> <a href="#simulacra-series" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Exploring Simulacra: A Tutorial Series on Simulacra’s Generative Agents</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_simulacra-series&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">March 31, 2024</span>
    
    
    

    

    
    <div id="objective_simulacra-series" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-final.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/abs/2304.03442" target="_blank">Park et al. Generative Agents: Interactice Simulacra of Human Behavior. UIST, 2023</a><br />
    <b>Task</b>: Creating and understanding generative agents proposed in Simulacra<br />
    <b>Libraries</b>: openai<br />
    <b>Series Overview</b>: This tutorial series spans four comprehensive notebooks, each designed to progressively build your understanding and skills in simulating sociological phenomena with generative agents. <a href="/blog/2024/simulacra-0/" target="_blank">Check out the blog post</a> where I break down the big ideas behind creating sociological simulations in an easy-to-get way.<br />
    <ul>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.1.ipynb" target="_blank">Notebook 1.1</a> - Basic Simulation Structure</b>: An introduction to the essential components of simulation, including environment interaction and the construction of a basic simulation loop.</li>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.2.ipynb" target="_blank">Notebook 1.2</a> - Building Generative Agents</b>: A deep dive into the creation of generative agents, with emphasis on memory structures derived from the Simulacra paper.</li>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.3.ipynb" target="_blank">Notebook 1.3</a> - Agent Schedule Planning</b>: Techniques for developing agents’ abilities to autonomously plan their schedules.</li>
      <li><b><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/simulacra/toy-simulacra-1.4.ipynb" target="_blank">Notebook 1.4</a> - Agent Cognition and Communication</b>: Exploration of agents’ cognitive functions, such as perception and memory retrieval, and how they interact and communicate.</li>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/pfn.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="pfn" class="highlight-text">
        <span> <a href="#pfn" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Prior-Fitted Networks</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_pfn&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Feb 5, 2024</span>
    
    
    

    

    
    <div id="objective_pfn" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-final.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/abs/2112.10510" target="_blank">Müller et al. Transformers can do Bayesian Inference. ICLR, 2022</a><br />
    <b>Task</b>: Using Transformers to estimate Posterior Predictive Distributions<br />
    <b>Libraries</b>: PyTorch<br />
    <b>Learning objectives</b>: <br />
    <ul>
      <li>Understand the principles of Prior-Data Fitted Networks (PFNs) and their application in integrating prior knowledge to predict the Posterior Predictive Distribution (PPD) in machine learning models.</li>
      <li>Acquire practical skills in defining priors, creating dataset loaders for synthetic data generation, developing transformer models for PPD approximation, formulating loss functions for specific regression tasks, and evaluating model output quality.</li>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/pfn_data.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="pfn-data-priors" class="highlight-text">
        <span> <a href="#pfn-data-priors" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Data Generating Priors for Prior-Fitted Networks</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_pfn-data-priors&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Feb 5, 2024</span>
    
    
    

    

    
    <div id="objective_pfn-data-priors" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/PFN/PFN-priors.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Müller et al. Transformers can do Bayesian Inference. ICLR, 2022</a><br /><a href="https://arxiv.org/abs/2207.01848" target="_blank">Hollmann et al. TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. ICLR, 2023</a><br />
    <b>Task</b>: Generating data according to different priors for PFNs<br />
    <b>Libraries</b>: PyTorch<br />
    <b>Learning objectives</b>: Learn how to generate synthetic data based on specified priors and utilize it for training neural networks to approximate Bayesian inference.<br />
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/llmtime.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="llm-time-series-forecasting" class="highlight-text">
        <span> <a href="#llm-time-series-forecasting" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>LLMTime - Zero-shot prompting LLMs for time series forecasting</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_llm-time-series-forecasting&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Nov 12, 2023</span>
    
    
    

    

    
    <div id="objective_llm-time-series-forecasting" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/LLMTime/LLM-timeseries-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/abs/2310.07820" target="_blank">Gruver et al. Large Language Models are Zero Shot Time Series Forecasters. NeurIPS, 2023</a><br />
    <b>Task</b>: Weather forecasting using LLMs<br />
    <b>Libraries</b>: openai, tiktoken, jax<br /><br />
    <b>Learning objectives</b>: Explore zero-shot prompting with Large Language Models (LLMs) for time series forecasting. In this tutorial, we aim to:<br />
    <ul>
      <li>Acquaint you with the application of machine learning techniques using Large Language Models (LLMs).</li>
      <li>Enhance your understanding of LLMs and the parameters influencing their behavior.</li>
      <li>Guide you through the essentials for successful time series prediction with LLMs.</li>
      <li>Translate knowledge from transformers to the realm of LLMs.</li>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/lora.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="critical-exploration-transformers" class="highlight-text">
        <span> <a href="#critical-exploration-transformers" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Critical Exploration of Transformer Models</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_critical-exploration-transformers&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Nov 7, 2023</span>
    
    
    

    

    
    <div id="objective_critical-exploration-transformers" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/transformer-extended-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Learning objectives</b>: Delve into the inner workings of transformer models beyond basic applications<br />
    <b>Key Areas</b>:
    <ul>
      <li>Adversarial Inputs: Crafting inputs to challenge language models</li>
      <li>Attention Visualization: Understanding focus mechanisms in transformers</li>
      <li>Fine-Tuning with LoRA: Implementing Low-Rank Adaptation for model refinement</li>
      <li>Bias Detection: Investigating biases in model responses</li>
    </ul>
    <b>Note</b>: This tutorial is an introductory exploration of transformers. It’s a starting point for more advanced study.<br /><br />
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/MAT.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="mat" class="highlight-text">
        <span> <a href="#mat" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Molecule Attention Transformer</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_mat&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Apr 21, 2023</span>
    
    
    

    

    
    <div id="objective_mat" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/molecule_attention_transformer/practical-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/pdf/2002.08264.pdf" target="_blank">Maziarka, et al. "Molecule Attention Transformer"</a><br />
    <b>Task</b>: Classification task to predict Blood-brain barrier permeability (BBBP)<br />
    <b>Dataset</b>: <a href="https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html#bbbp-datasets" target="_blank">BBBP</a><br />
    <b>Libraries</b>: PyTorch, DeepChem, RDKit <br /><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Learn key concepts required to work with molecules</li>
      <li>Perform critical data preprocessing tasks, such as feature extraction, graph formation, and scaffold splitting</li>
      <li>Explore challenges of drug discovery, particularly designing drugs that can cross the blood-brain barrier and enter the central nervous system</li>
      <li>Implement Molecule Attention Transformer (MAT) proposed by Maziarka et al. (2020) using a deep learning pipeline</li>
      <li>Train and evaluate the model on molecular datasets</li>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/ard.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="ard" class="highlight-text">
        <span> <a href="#ard" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Accessing Research Data for Social Science [Oxford Internet Institute, MT 2022]</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_ard&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">September, 2022</span>
    
    
    

    

    
    <div id="objective_ard" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
            <span class="h1-links"><a href="https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
        </h1>
      </p>
        
    <b>DeepNote</b>: (Jupyter notebook hosting service) <a href="https://deepnote.com/project/Accessing-Research-Data-Assignments-90bjc9xBTb6z-V869C_2WA">DIY Notebooks</a>.<br />
    <b>Github</b>: <a href="https://github.com/pg2455/accessing-research-data">Repository</a> to work on your local machine.<br />
    <b>Programming Language</b>: Python<br />
    <b>Libraries</b>: Pandas, feedparser, newscatcherapi, psaw, requests, twarc (Twitter API), requests-html<br />

    <b>Learning objectives</b>:
    <ul>
      <li>Use Python to collect research data from the social web</li>
      <li>Give due consideration to the ethics of data collection</li>
      <li> Following topics are covered:</li>
      <ul>
        <li>Accessing RSS feeds</li>
        <li>Accessing Reddit and Wikipedia through APIs</li>
        <li>Accessing Twitter using Twitter API</li>
        <li>Web crawling</li>
      </ul>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/transformer_architecture.jpeg" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="attn" class="highlight-text">
        <span> <a href="#attn" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Attention is all you need</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_attn&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Aug 5, 2021</span>
    
    
    

    

    
    <div id="objective_attn" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/tutorials/attention_is_all_you_need/practical-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Publication</b>: <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">Vaswani, Ashish, et al. "Attention is all you need." NeurIPS 2017.</a><br />
    <b>Task</b>: Neural Machine Translation (e.g, German-English)<br />
    <b>Dataset</b>: <a href="https://arxiv.org/abs/1605.00459" target="_blank">Multi30k</a><br />
    <b>Libraries</b>: PyTorch, NLTK, Spacy, torchtext <br /><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Build a transformer model for neural machine translation</li>
      <li>Train the model using proposed label smoothing loss and learning rate scheduler</li>
      <li>Use the trained model to infer likely translations using </li>
        <ul>
          <li>Greedy Decoding</li>
          <li>Beam Search</li>
        </ul>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li></ol>

</table>

<h2 id="-courses"><i class="fa fa-chevron-right"></i> Courses</h2>

<table class="table table-hover">

  <ol class="bibliography"><li><tr>
  <td class="col-md-3"><img src="../images/resources/attention.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="attn" class="highlight-text">
        <span> <a href="#attn" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Attention, Transformers, LLMs</strong><br />
      </div>

    
      <span> [<a href="javascript:;" onclick="$(&quot;#lessons_attn&quot;).toggle()">Topics</a>] </span>
    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#practical_attn&quot;).toggle()">Practical</a>] </span>
    

    

    
    
    
    <div id="lessons_attn" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Topics
          
        </h1>
        
    <ul>
      <li>Attention: Applications (2014-2017) <a href="https://github.com/pg2455/ml_resources/blob/master/courses/attention_to_llms/attention_applications.ipynb" target="_blank">[Notebook]</a></li>
      <li>Transformers (2017-2022) <a href="https://github.com/pg2455/ml_resources/blob/master/courses/attention_to_llms/effecient_transformers.ipynb" target="_blank">[Notebook]</a></li>
      <li>LLMs (2020-2023) <a href="https://github.com/pg2455/ml_resources/blob/master/courses/attention_to_llms/LLMs.ipynb" target="_blank">[Notebook]</a></li>
    </ul>
    
      </p>
    </div>
    

    
    <div id="practical_attn" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Practical
          
          
        </h1>
        
    Tutorial on <a href="tutorials#attn">Attention is all you need</a> (Vaswani et al. 2017).
    
      </p>
    </div>
    

    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/imagenet.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="mna" class="highlight-text">
        <span> <a href="#mna" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Advanced Convolutional Neural Network Architectures (2012 - 2018)</strong><br />
      </div>

    
      <span> [<a href="javascript:;" onclick="$(&quot;#lessons_mna&quot;).toggle()">Topics</a>] </span>
    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#practical_mna&quot;).toggle()">Practical</a>] </span>
    

    

    
    
    
    <div id="lessons_mna" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Topics
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/mna/lessons/mna-interactive.ipynb" target="_blank">[ Slides/Notebook ]</a></span>
          
        </h1>
        
    <ul>
      <li>History of vision architectures (1998-2012)</li>
      <li>Network in Network (2014)</li>
      <li>InceptionNet (2014)</li>
      <li>ResNet (2015)</li>
      <li>Pre-activations: Improved ResNet (2016)</li>
      <li>DenseNet (2016)</li>
      <li>WaveNet (2016)</li>
      <li>Depthwise Separable Convolutions (2017)</li>
      <li>Squeeze-and-Excite: ResNet (2017)</li>
      <li>Neural Architecture Search (2018 - Present)</li>
    </ul>
    
      </p>
    </div>
    

    
    <div id="practical_mna" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Practical
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/mna/practical/mna-practical.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/mna/practical/mna-practical-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
        
    <b>Task</b>: Build an image classifier<br />
    <b>Dataset</b>: <a href="https://www.cs.toronto.edu/ kriz/cifar.html" target="_blank"> CIFAR-10</a><br />
    <b>Libraries</b>: PyTorch, torchvision <br /><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Build and train <a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank">InceptionNet</a> on CIFAR-10</li>
      <li>Build and train <a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank">DenseNet</a> on CIFAR-10</li>
    </ul>

    
      </p>
    </div>
    

    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/pruning.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="pruning" class="highlight-text">
        <span> <a href="#pruning" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Network Pruning</strong><br />
      </div>

    
      <span> [<a href="javascript:;" onclick="$(&quot;#lessons_pruning&quot;).toggle()">Topics</a>] </span>
    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#practical_pruning&quot;).toggle()">Practical</a>] </span>
    

    

    
    
    
    <div id="lessons_pruning" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Topics
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/network-pruning/lessons/network-pruning-slides-interactive.ipynb" target="_blank">[ Slides/Notebook ]</a></span>
          
        </h1>
        
    <ul>
      <li>Motivation</li>
      <li>Network Pruning: Pipeline</li>
      <li>Network Pruning: Unstructured vs Structured</li>
      <li>Network Pruning: Criterion</li>
      <li>Network Pruning: Prune Rate</li>
      <li>Network Pruning: Lottery Ticket Hypothesis</li>
      <li>Knowledge Distillation</li>
      <li>Quantization</li>
    </ul>
    
      </p>
    </div>
    

    
    <div id="practical_pruning" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Practical
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/network-pruning/practical/pruning-practical.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/network-pruning/practical/pruning-practical-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
        
    <b>Task</b>: Build a land use classifier<br />
    <b>Dataset</b>: <a href="https://github.com/phelber/eurosat" target="_blank">EuroSat (RGB)</a><br />
    <b>Libraries</b>: PyTorch, torchvision <br /><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Build and train a simple classifier</li>
      <li>Use different techniques of pruning available in PyTorch to reduce the size of the network</li>
      <li>Observe the effect of pruning on model accuracy</li>
    </ul>

    
      </p>
    </div>
    

    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/ae.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="dae" class="highlight-text">
        <span> <a href="#dae" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Deep Autoencoders and Variational Autoencoders</strong><br />
      </div>

    
      <span> [<a href="javascript:;" onclick="$(&quot;#lessons_dae&quot;).toggle()">Topics</a>] </span>
    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#practical_dae&quot;).toggle()">Practical</a>] </span>
    

    

    
    
    
    <div id="lessons_dae" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Topics
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/deep-autoencoders/lessons/deep-autoencoders-slides-interactive.ipynb" target="_blank">[ Slides/Notebook ]</a></span>
          
        </h1>
        
    <ul>
      <li>Autoencoders: Motivation &amp; History</li>
      <li>Autoencoders: Loss function</li>
      <li>Undercomplete Autoencoders</li>
      <li>Overcomplete Autoencoders</li>
      <li>Stacked / Deep Autoencoders</li>
      <li>Variational Autoencoders</li>
      <li>Applications of Autoencoders</li>
    </ul>
    
      </p>
    </div>
    

    
    <div id="practical_dae" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Practical
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/deep-autoencoders/practical/deep-autoencoders-pratical.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/deep-autoencoders/practical/deep-autoencoders-pratical-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
        
    <b>Task</b>: Explore the use of autoencoders for
    <ul>
      <li>Data compression</li>
      <li>Data generation</li>
      <li>Data interpolation</li>
    </ul>
    <b>Dataset</b>: <a href="http://yann.lecun.com/exdb/mnist/" target="_blank"> MNIST</a><br />
    <b>Libraries</b>: PyTorch, torchvision <br /><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Build and train the following types of autoencoders</li>
      <ul>
        <li>Sparse autoencoder with L1 penalty</li>
        <li>Sparse autoencoder with KL penalty</li>
        <li>Contractive Autoenoder</li>
        <li>Variational Autoenoder</li>
      </ul>
      <li>Explore the quality of their learned latent space</li>
    </ul>
    
      </p>
    </div>
    

    

  </div>
  </td>
</tr>
</li>
<li><tr>
  <td class="col-md-3"><img src="../images/resources/watermarks.png" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="nr" class="highlight-text">
        <span> <a href="#nr" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Noise Reduction in Machine Learning</strong><br />
      </div>

    
      <span> [<a href="javascript:;" onclick="$(&quot;#lessons_nr&quot;).toggle()">Topics</a>] </span>
    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#practical_nr&quot;).toggle()">Practical</a>] </span>
    

    

    
    
    
    <div id="lessons_nr" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Topics
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/noise-reduction/lessons/noise-reduction-slides-interactive.ipynb" target="_blank">[ Slides/Notebook ]</a></span>
          
        </h1>
        
    <ul>
      <li>Sources of noise</li>
      <li>Impact of noise</li>
      <li>Examples of noise</li>
      <li>Noise reduction techniques</li>
      <ul>
        <li>Rolling Window</li>
        <li>Convolution</li>
        <li>Digital Filters</li>
        <li>Machine Learning Fitlers</li>
        <li>Denoising Autoencoders</li>
      </ul>
    </ul>
    
      </p>
    </div>
    

    
    <div id="practical_nr" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">Practical
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/noise-reduction/practical/noise-reduction-practical.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://github.com/pg2455/ml_resources/blob/master/courses/noise-reduction/practical/noise-reduction-practical-solution.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
        
    <b>Task</b>: Denoise corrupted images<br />
    <b>Dataset</b>: <a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank">Fashion MNIST</a><br />
    <b>Libraries</b>: PyTorch, torchvision <br /><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Build a simple Denoising Autoencoder (DAE) for images</li>
      <li>Train such a DAE on images with a given corrupted noise </li>
      <li>Visualize the quality of reconstruction</li>
    </ul>
    
      </p>
    </div>
    

    

  </div>
  </td>
</tr>
</li></ol>

</table>

<h2 id="-hackathons"><i class="fa fa-chevron-right"></i> Hackathons</h2>

<table class="table table-hover">

  <ol class="bibliography"><li><tr>
  <td class="col-md-3"><img src="../images/resources/covid.jpeg" onerror="this.style.display='none'" /></td>
  <td>
    <div class="content-text">
      <div id="covid" class="highlight-text">
        <span> <a href="#covid" target="_blank"><i class="fas fa-link"></i></a> </span>
        <strong>Prediction of COVID Infection using reported symptoms</strong><br />
      </div>

    

    

    
      <span> [<a href="javascript:;" onclick="$(&quot;#objective_covid&quot;).toggle()">Details</a>] </span>
    

    
      <span style="float: right; color:grey;font-size:1em">Aug 8, 2021</span>
    
    
    

    

    
    <div id="objective_covid" class="hidden-content" style="text-align: justify; display: none">
      <p>
        <h1 class="resources-headline">&nbsp;
          
          
            <span class="h1-links"><a href="https://colab.research.google.com/gist/pg2455/cfeda1402bb2369d718ce4bd7140f19d/copy-of-covid-diagnosis-hackathon.ipynb" target="_blank">  [ Notebook (DIY) ] </a></span>
          
          
            <span class="h1-links"><a href="https://colab.research.google.com/gist/pg2455/e9f8b7ca5f807458d24230d02f8e81a2/copy-of-covid-diagnosis.ipynb" target="_blank"> [ Notebook (Soln.) ] </a></span>
          
        </h1>
      </p>
        
    <b>Based on</b>: <a href="https://www.nature.com/articles/s41746-020-00372-6" target="_blank">Zoabi et al. "Machine learning-based prediction of COVID-19 diagnosis based on symptoms." npj digital medicine 4.1 (2021): 1-5.</a><br />
    <b>Task</b>: Predict COVID-19 infection from reported symptoms<br />
    <b>Dataset</b>: <a href="https://github.com/nshomron/covidpred" target="_blank">English translation of COVID infections reported by Israeli Ministry of Health</a><br />
    <b>Learning objectives</b>:
    <ul>
      <li>Explore a realistic dataset and prepare it for building a practical machine learning system</li>
      <li>Think through various practical issues related to deploying such a system (e.g., class imbalance, data collection bias)</li>
      <li>Build and train such an ML system addressing the issues discovered above</li>
    </ul>
    
    </div>
    

  </div>
  </td>
</tr>
</li></ol>

</table>

</div>
</div>

  <script type="text/javascript">

    var elems = document.querySelectorAll("ol.bibliography");
    for (var i = 0; i < elems.length; i++) {
       elems[i].remove();
    }

    var elems = document.querySelectorAll("li:empty");
    for (var i = 0; i < elems.length; i++) {
       elems[i].remove();
    }


  </script>
</div>


  <script src="/js/sp.js"></script>
  <script src="/js/main.js"></script>
  <script src="/vendor/js/jquery.min.js"></script>
  <script src="/vendor/js/bootstrap.min.js"></script>
  <script src="/vendor/js/anchor.min.js"></script>
  <script src="/vendor/js/jquery.toc.js"></script>
  <script type="text/javascript">
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
   })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

   ga('create', 'UA-187458159-1', 'auto');
   ga('send', 'pageview');

   $("#toc").toc({
       'headings': 'h2,h3'
   });
   anchors.add('h2,h3');
  </script>

  <script>
    document.addEventListener("DOMContentLoaded", function() {
        // Check if there is a hash in the URL
        const hash = window.location.hash;
        if (hash) {
            // Find the element that matches the hash
            const targetElement = document.querySelector(hash);
            if (targetElement) {
                // Add the highlight class to the element
                targetElement.classList.add('highlight');
                
                // If there are collapsible sections related to this hash, open them
                const relatedSections = ['lessons', 'practical', 'objective'];
                relatedSections.forEach(section => {
                    const elem = document.getElementById(section + '_' + hash.slice(1));
                    if (elem) {
                        elem.style.display = 'block'; // Change display style to 'block' to show the section
                    }
                });

                // Set a timeout to remove the highlight after 5 seconds
                setTimeout(() => {
                    targetElement.classList.remove('highlight');
                }, 5000); // 5000 milliseconds = 5 seconds
            }
        }
    });

</script>

</body>

</html>
